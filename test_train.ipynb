{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from Feat2Annot import *\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torcheval import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "689459it [00:00, 2059662.13it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': 7, 'weight': array([0.00265764, 0.04788601, 0.01087034, 0.00688993, 0.01129333,\n",
      "       0.91583195, 0.0045708 ])}\n"
     ]
    }
   ],
   "source": [
    "from util import prepare_dataset\n",
    "\n",
    "dataset = prepare_dataset(\"./data\", 15)\n",
    "print(dataset.get_annot_class())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, (0.9, 0.1), generator)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Feat2AnnotModel(\n",
    "    input_size=200, hidden_size=1024, target_class=dataset.get_annot_class()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initial model score\n",
    "metric = metrics.MulticlassAccuracy()\n",
    "metric.reset()\n",
    "model.eval()\n",
    "for source_feature, tgt_annot in tqdm(val_loader):\n",
    "    annot_hypothesis = model.beam_search(source_feature, 1)\n",
    "    annot_hat = torch.tensor(annot_hypothesis[0].value, dtype=torch.int64)\n",
    "    metric.update(annot_hat, tgt_annot.squeeze(0))\n",
    "init_valid_metric = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [04:35<00:00,  2.91it/s]\n"
     ]
    }
   ],
   "source": [
    "### Run one episode of training\n",
    "metric = metrics.MulticlassAccuracy()\n",
    "num_epoch = 1\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "val_metric = []\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for epoch in range(num_epoch):\n",
    "    metric.reset()\n",
    "    model.train()\n",
    "    for xbatch, ybatch in tqdm(train_loader):\n",
    "        output = model(xbatch, ybatch)\n",
    "        loss = -output\n",
    "        batch_loss = loss.sum()\n",
    "        loss = batch_loss / batch_size\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for xval_batch, yval_batch in tqdm(val_loader):\n",
    "    #         output = model(xval_batch, yval_batch)\n",
    "    #         val_loss = -output.sum() / batch_size\n",
    "    #         annot_hypothesis = model.beam_search(xval_batch, 1)\n",
    "    #         annot_hat = torch.tensor(annot_hypothesis[0].value, dtype=torch.int64)\n",
    "    #         metric.update(annot_hat, yval_batch.squeeze(0))\n",
    "    #     val_metric.append(metric.compute())\n",
    "\n",
    "    # print(\n",
    "    #     f\"epoch {epoch+1}/{num_epoch}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\"\n",
    "    # )\n",
    "    training_losses.append(loss.item())\n",
    "    # val_losses.append(val_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 0, 1]])\n",
      "\n",
      "Bleeding Result:\n",
      "tensor([[[0, 1, 1, 1, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 0]],\n",
      "\n",
      "        [[1, 1, 1, 1, 0, 1, 1, 1, 1]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Example tensor with 0s and 1s\n",
    "tensor = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 1, 1, 1, 0, 1, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0, 1, 0, 1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a convolutional kernel to perform the bleeding operation\n",
    "kernel = torch.tensor([[1, 1, 1]])\n",
    "\n",
    "# Perform 1D convolution with the kernel along each row of the tensor\n",
    "result = torch.nn.functional.conv1d(\n",
    "    tensor.unsqueeze(1).float(), kernel.unsqueeze(0).float(), padding=1\n",
    ").squeeze(0)\n",
    "\n",
    "# Convert the result to binary tensor by thresholding\n",
    "result_binary = (result > 0).int()\n",
    "\n",
    "print(\"Original Tensor:\")\n",
    "print(tensor)\n",
    "print(\"\\nBleeding Result:\")\n",
    "print(result_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"naive model val accuracy {init_valid_metric}\")\n",
    "print(f\"after one episode of training {val_metric[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target tensor([[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0]])\n",
      "predicted tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "new_val_loader = DataLoader(\n",
    "    dataset=val_loader.dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "dataiter = iter(new_val_loader)\n",
    "source_feature, target_seq = next(dataiter)\n",
    "annot_hypothesis = model.beam_search(source_feature, 1)\n",
    "annot_hat = torch.tensor(annot_hypothesis[0].value, dtype=torch.int64)\n",
    "print(f\"target {target_seq}\")\n",
    "print(f\"predicted {annot_hat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load params\n",
    "# load model\n",
    "params = torch.load(\"model.bin\", map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(params[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0],\n",
    "    ]\n",
    ")\n",
    "b, c, d = exponential_weight(a, 0.25)\n",
    "from matplotlib import pyplot\n",
    "\n",
    "fig, axes = pyplot.subplots(1, 4)\n",
    "axes[0].plot(b[1, :])\n",
    "axes[1].plot(a[1, :])\n",
    "axes[2].plot(c[1, :])\n",
    "axes[2].plot(d[1, :])\n",
    "print(c[0, :], d[1, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoanot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
